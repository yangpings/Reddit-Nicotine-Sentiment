{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1: Scrape comments and posts from Reddit",
   "id": "3028471d295e44b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### import libraries and setup reddit app from client interfaces",
   "id": "d1e532f3e7e64702"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#import libraries and sensitive information\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from reddit_auth0 import client_id, client_secret, username, password"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#setup reddit\n",
    "reddit = praw.Reddit(\n",
    "    user_agent = True,  client_id =client_id,\n",
    "    client_secret = client_secret,\n",
    "    username = username, password = password)"
   ],
   "id": "84002e5c06ba3a5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get {number of posts} from each {keyword}. In this notebook, 4 keywords 50 posts each",
   "id": "608d65c7fb271547"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_reddit_posts(keyword, limit):\n",
    "    \"\"\"Fetch top relevant post URLs based on keyword and limit\"\"\"\n",
    "    search_results = reddit.subreddit(\"all\").search(keyword, sort=\"relevance\", limit=limit)\n",
    "    \n",
    "    urls = [post.url for post in search_results]\n",
    "    keywords = [keyword] * len(urls)  # Create an array with the keyword repeated\n",
    "    \n",
    "    return urls, keywords  # Returning both lists\n",
    "\n",
    "# Storage for accumulating multiple searches\n",
    "urls = []\n",
    "keywords = []\n",
    "\n",
    "# Main loop for user input\n",
    "while True:\n",
    "    keyword = input(\"\\nEnter a keyword to search Reddit (or type 'exit' to stop): \").strip()\n",
    "    \n",
    "    if keyword.lower() == \"exit\":\n",
    "        print(\"\\nExiting program. Here are all the collected URLs and keywords:\\n\")\n",
    "        print(\"URLs:\", urls)\n",
    "        print(\"Keywords:\", keywords)\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        num_posts = int(input(\"Enter the number of top posts to fetch: \").strip())\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid number.\")\n",
    "        continue\n",
    "\n",
    "    # Fetch and store results\n",
    "    new_urls, new_keywords = fetch_reddit_posts(keyword, num_posts)\n",
    "\n",
    "    urls.extend(new_urls)  # Append new URLs to the existing list\n",
    "    keywords.extend(new_keywords)  # Append new keywords to the existing list\n",
    "\n",
    "    # Display results after each search\n",
    "    print(\"\\nCurrent Collected Data:\")\n",
    "    print(\"URLs:\", urls)\n",
    "    print(\"Keywords:\", keywords)"
   ],
   "id": "8400f0e422770d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### actual number of posts saved ",
   "id": "9f5db904478be75f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(keywords)",
   "id": "a1a60a1a051754cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(urls)",
   "id": "3152885b913e89f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### delete dupliucated links and media posts (images and videos). In this case, we only use posts with words only",
   "id": "d80f112412d79464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seen = set()\n",
    "unique_urls = []\n",
    "unique_keywords = []\n",
    "\n",
    "for url, keyword in zip(urls, keywords):\n",
    "    if url.startswith(\"https://www.reddit.com/\") and url not in seen:\n",
    "        unique_urls.append(url)\n",
    "        unique_keywords.append(keyword)  # Keep the corresponding keyword\n",
    "        seen.add(url)\n",
    "\n",
    "# Overwrite the original lists\n",
    "urls = unique_urls\n",
    "keywords = unique_keywords\n",
    "\n",
    "# Output results\n",
    "print(\"Unique URLs after filtering and removing duplicates:\")\n",
    "print(urls)\n",
    "print(\"Length of URLs:\", len(urls))  \n",
    "\n",
    "print(\"\\nKeywords aligned with unique URLs:\")\n",
    "print(keywords)\n",
    "print(\"Length of Keywords:\", len(keywords))"
   ],
   "id": "6f64e4f94aeb2320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract the posts",
   "id": "1c9928da3935775c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "post_log = []",
   "id": "f040b3fb824c6b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#fetch comments, posts, and replies\n",
    "i = 0\n",
    "while i < len(urls) - 1:\n",
    "    post = reddit.submission(url = urls[i])\n",
    "    post_log.append({\n",
    "        \"author\": post.author,\n",
    "        \"time_posted\": datetime.fromtimestamp(post.created_utc),\n",
    "        \"title\": post.title,\n",
    "        \"post\": post.selftext,\n",
    "        \"up-votes\": post.score,\n",
    "        \"subreddit\": post.subreddit,\n",
    "        \"number_of_comments\": len(post.comments),\n",
    "        \"keyword\": keywords[i],\n",
    "        \"post_number\": i+1\n",
    "    })\n",
    "    print(f\"post {i} extracted \")\n",
    "    i = i+1"
   ],
   "id": "48f9ada680df2488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(post_log)\n",
    "df"
   ],
   "id": "ef6da6ac427abe34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sum(df['number_of_comments'])",
   "id": "b0aab7c2b75799bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract the comments and replies",
   "id": "41aee21f934ff51d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "comment_datalog = []",
   "id": "98cba5ee21646d57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(urls))",
   "id": "ea8b3a51fe6766c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "i = 0\n",
    "while i < len(urls) - 1:\n",
    "    post = reddit.submission(url = urls[i])\n",
    "    post.comments.replace_more(limit=None)\n",
    "    for comment in post.comments:\n",
    "        comment_datalog.append({\n",
    "            \"author\": comment.author,\n",
    "            \"time_posted\": datetime.fromtimestamp(comment.created_utc),\n",
    "            \"post\": comment.body,\n",
    "            \"up-votes\": comment.score,\n",
    "            \"comment\": 1,\n",
    "            \"reply\": 0,\n",
    "            \"keyword\": keywords[i],\n",
    "            \"post_number\": i+1\n",
    "        })\n",
    "        for reply in comment.replies:\n",
    "            comment_datalog.append({\n",
    "            \"author\": reply.author,\n",
    "            \"time_posted\": datetime.fromtimestamp(reply.created_utc),\n",
    "            \"post\": reply.body,\n",
    "            \"up-votes\": reply.score,\n",
    "            \"comment\": 0,\n",
    "            \"reply\": 1,\n",
    "            \"keyword\": keywords[i],\n",
    "            \"post_number\": i+1\n",
    "        })\n",
    "    print(f\"post {i} done.\")\n",
    "    i = i + 1   "
   ],
   "id": "176719822e6eb0de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(reddit.auth.limits)",
   "id": "2cd5f6839c790df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df2 = pd.DataFrame(comment_datalog)\n",
    "print(df2.shape)\n",
    "df2"
   ],
   "id": "43d27435084cd703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df2 = df2.drop_duplicates(subset='text', keep='first')\n",
    "print(df2.shape)"
   ],
   "id": "db86bc1c36621e50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "index = df2['post_number'].unique()",
   "id": "beac07e56cf8b79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = df[df['post_number'].isin(index)]",
   "id": "e9a9e2a6340e5744",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Export into csv files",
   "id": "3787d8cdf1525244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.to_csv('reddit_post_log.csv', index=False)\n",
    "df2.to_csv('reddit_comment_log.csv', index=False)"
   ],
   "id": "75e3fb8fbfbe6996",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## combine comments and posts together",
   "id": "86561b0981e42694"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "75a2e3d2fbe59aff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "post_log = pd.read_csv(\"reddit_post_log.csv\")\n",
    "print(post_log.shape)\n",
    "post_log.head()"
   ],
   "id": "6165ae36935b4458",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comment_log = pd.read_csv(\"reddit_comment_log.csv\")\n",
    "print(comment_log.shape)\n",
    "comment_log.head()"
   ],
   "id": "6a2537e3fbf464c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "post_log['post'] = post_log['post'].fillna(post_log['title'])\n",
    "post_log.head()"
   ],
   "id": "55bbfaf6a8575978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comment_log.rename(columns={'text': 'post'}, inplace=True)\n",
    "comment_log.rename(columns={'Up-votes': 'up-votes'}, inplace=True)\n",
    "comment_log.rename(columns={'Author': 'author'}, inplace=True)\n",
    "comment_log.head()"
   ],
   "id": "8b7b216144183db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "common_columns = post_log.columns.intersection(comment_log.columns)\n",
    "post_log_common = post_log[common_columns]\n",
    "comment_log_common = comment_log[common_columns]\n",
    "general_log = pd.concat([post_log_common, comment_log_common], ignore_index=True)\n",
    "print(general_log.shape)\n",
    "general_log"
   ],
   "id": "b2fc3bc9f987916c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "general_log.to_csv('reddit_post_comment_log.csv', index=False)",
   "id": "e43671429091a9f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
